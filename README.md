# ðŸ“Œ HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals

Arxiv: https://arxiv.org/pdf/2507.13318? (Findings of EMNLP2025) 

---

## ðŸ“– Introduction
**HapticCap** is a multimodal dataset and benchmark task designed for understanding **user experience of vibration-based haptic signals**.  
It provides a new resource for research at the intersection of **haptics, text, and multimodal learning**.

---

## ðŸ“‚ Dataset
- **Modality**: Vibration haptic signals, paired with textual annotations  
- **Textual Annotations**:  
  - Sensory: It refers to physical attributes (e.g.,intensity of tapping).
  - Emotional: It refers to emotional denotes affective impressions (e.g., the mood of a scene).
  - Associative: It indicates real-world familiar experiences (e.g., buzzing of a bee, a heartbeat).
 
- **Format**: Signals stored as time-series data; annotations in JSON with haptic signal ID  
- **Scale**: 

---

Google drive:

Haptic Vibration Signals:

Human Descriptions:


## ðŸ§© Tasks
- Haptic-caption retrieval

- Training set:

- Valid set:

- Test set:

## ðŸ§© Models



---

## ðŸš€ Resource:







